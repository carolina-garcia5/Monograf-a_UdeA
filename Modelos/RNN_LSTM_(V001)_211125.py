# -*- coding: utf-8 -*-
"""EN_RNN_CarolinaGarcíaP_(V001).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cqYwNUMevH3ngaGOVyohP8WNUXVgicKF

<p><img alt="Colaboratory logo" height="10px" src="https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg" align="left" hspace="10px" vspace="0px",width="100"></p>



<center><h1> Primera Entrega Monografía</h1>
<h2> Carolina García Patiño</h2>
<h3> C.C 1152438081</h3></center>

<br>
<font color='gray'> Problem description, Data cleaning and first models attempts</font>

# TABLE OF CONTENTS

1. [Problem Description](#id1)
2. [Import dataset](#id2)
3. [Data Exploration](#id3)
4. [Preprocessing](#id4)<br>
    4.1 [Imbalaced classes](#id5)<br>
    4.2 [Data cleaning](#id6)<br>
    4.3 [Count Unique words](#id7)<br>
    4.4. [Duplicated and null data](#id8)<br>
    4.5 [Delete duplicated and null data](#id9)
5. [Amount of representative words](#id10)<br>
    5.1[Pareto Analysis](#id11)
6. [Model](#id12)<br>
    6.1 [Multinomial Naive  Bayes](#id3)<br>
      6.1.1. [Evaluation Naive Bayes](#id14)<br>
    6.2 [RandomForest Classifier](#id15)<br>
      6.2.1. [Evaluation Random Forest Classifier](#id16)

<div id='id1'/>

# 1) Problem Description

The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee[1].<br>
Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews

**The sentiment labels are:**

0 - negative <br>
1 - somewhat negative<br>
2 - neutral<br>
3 - somewhat positive<br>
4 - positive<br>

**Import Libraries**
"""

### LIBRERÍAS ESTÁNDAR ###
import csv
import pandas as pd
import numpy as np

## LIBRERÍAS GRÁFICAS ##
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme()

#import googletrans 
#from langdetect import detect

"""<div id='id2'/>

# 2) Import dataset
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive/Colab Notebooks/Monografía_Especialización"

dftrain = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Monografía_Especialización/train.tsv.zip",sep='\t')
dftrain.head()

print(f'Dataset train rows:{dftrain.shape[0]}')

dftrain.dtypes

"""After apply cleanning process, there was an amount of duplicated values including n as product of the descomposition of the word n't. Due to, that word goin to be replace it for the all negation as not."""

dftrain['Phrase'].replace(to_replace=r"\bn't\b", value='not', regex=True, inplace= True)

dftrain['Sentiment'].unique()

"""<div id='id3'/>

# 3) Data Exploration

Let see some generally for the columns

**SentenceId**
"""

len(dftrain.SentenceId.unique().tolist())

"""It seems this column could be the identifier for the setences, but there are fewer unique register than total rows.

**Phrase**
"""

plt.hist(dftrain.Phrase.isna().sum())
plt.title('Missing values in Phrase before preprocessing')
plt.show()

print(f'Nun Values in phrase are:{dftrain.Phrase.isnull().sum()}')

print(f'Nun Values in phrase are:{dftrain.Phrase.isna().sum()}')

"""There are not empty phrases in the dataframe"""

plt.hist(dftrain.duplicated(subset=['Phrase']).sum())
plt.title('Duplicated values in Phrase before preprocessing')
plt.show()

"""- Identify phrase's len"""

df_phrase_senti = dftrain[['Phrase','Sentiment']]

df_phrase_senti['Len'] = df_phrase_senti.apply(lambda row: len(row.Phrase), axis=1)

df_phrase_senti.head()

lens = df_phrase_senti.Phrase.str.len()
lens.mean(), lens.std(), lens.max()

lens.hist()
plt.title('Phrases len before preprocessing-Including spaces')

"""The phrases's len withot doing some preprocessing work is around 40 words, that includes the sapces in the phrase. Apparently there no duplicated information.

- Lets take a look phrase's len without spaces
"""

df_phrase_senti['Len_not_spaces'] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row: len(row.replace(" ", "")))
df_phrase_senti['Len_not_spaces'].mean(), df_phrase_senti['Len_not_spaces'].std(), df_phrase_senti['Len_not_spaces'].max()

lens2 = df_phrase_senti.Phrase.replace(" ","").str.len()
lens2.hist()
plt.title('Phrases len without spaces')

"""The phrases's len withot spaces is around 34 words, the distribution is similary that the previous one. Now, appears 62 duplicated phrases."""

df_phrase_senti.Phrase.str.replace(" ","").isnull().sum()

plt.hist(df_phrase_senti.Phrase.str.replace(" ","").isnull().sum())
plt.title('Missing values in without spaces')
plt.show()

plt.hist(df_phrase_senti.Phrase.str.replace(" ","").duplicated().sum())
plt.title('Duplicated values in without spaces')
plt.show()

tmp = df_phrase_senti.Phrase.str.replace(" ","")
tmp.duplicated(keep= False).sum()

"""- Count words in a sentence"""

df_phrase_senti['amount_words'] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row: len(row.split()))

df_phrase_senti['amount_words'].hist()
plt.title('Amount of words without preprocessing')

df_phrase_senti['amount_words'].mean()

df_phrase_senti[df_phrase_senti['amount_words']==0]

"""The average amount of words is 7 per phrase.<br>
There is a one register without no words.

- Identify Language
"""

def language_detect(sentence):
    try:
        language = detect(sentence)
    except:
        language= "error"
    return language

#df_phrase_senti["Language"] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row: language_detect(row))
#df_phrase_senti.shape

#df_phrase_senti.Language.value_counts()

#df_phrase_senti[df_phrase_senti["Language"] =='it'].head()

#df_phrase_senti[df_phrase_senti["Language"] =='fr'].head()

#df_phrase_senti[df_phrase_senti["Language"] =='fr'].Len.unique()

"""**Sentiment**"""

print(f'Nun Values in Sentiment are:{dftrain.Sentiment.isnull().sum()}')

dftrain.Sentiment.value_counts().to_frame().plot(kind = 'bar')

dftrain.Sentiment.value_counts()

"""There is a problem for imbalace classes. The neutral class represent 51%, 21% some positive ,17% some negative, 6% positive and 5% negative.

The strategy would be two for solve the problem:
1) To add the negative and some negative classes into negative and positives and some positive into positive. <br>
2) Implemate Smoteen strategy, creating sintetic samples, to balance classes.

<div id='id4'/>

# 4) Preprocessing

<div id='id5'/>

## 4.1. Imbalaced classes
"""

plt.figure(figsize=(4, 4))
dftrain.Sentiment.value_counts().to_frame().plot(kind = 'bar', color='teal')
plt.show()

tmp_class = pd.DataFrame(dftrain.Sentiment.value_counts()).sort_index()
tmp_class['Percentage'] = round(tmp_class['Sentiment']/sum(tmp_class['Sentiment'])*100,2)
tmp_class

def balanced_classes(class_init):
    if class_init == 0:
        value = 1
    elif class_init == 4:
        value = 3
    else:
        value = class_init
        
    return value

df_phrase_senti['Sentiment_balanced'] = df_phrase_senti.loc[:,'Sentiment'].apply(lambda row: balanced_classes(row) )

plt.figure(figsize=(4, 4))
df_phrase_senti['Sentiment_balanced'].value_counts().to_frame().plot(kind = 'bar', color='skyblue')
plt.xlabel('Class')
plt.show()

tmp2_class = pd.DataFrame(df_phrase_senti.Sentiment_balanced.value_counts()).sort_index()
tmp2_class['Percentage'] = round(tmp2_class['Sentiment_balanced']/sum(tmp2_class['Sentiment_balanced'])*100,2)
tmp2_class

dftrain['Sentiment_balanced'] = dftrain.loc[:,'Sentiment'].apply(lambda row: balanced_classes(row) )

"""<div id='id6'/>

## 4.2. Data cleaning

In a first attempt, the data cleaning process only takes remong stopwords, but this process let some wordas as n't, that are identifued as a contraction of a negation word in english lenguage. For that reason, in a second attempt lemmatization is implemented to reduce that kind of issue.
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

nltk.download('wordnet')

import re
from nltk.stem import WordNetLemmatizer

my_stopwords = list(stopwords.words('english'))

my_stopwords[:10]

import string 
punctuation = list(string.punctuation)

"""Add some extra special characteres extract from a previous exploration in the counter words fase."""

punctuation.append('--')
punctuation.append('...')
punctuation.append('``')
punctuation.append('')

punctuation[:10]

final_stopwords = my_stopwords + punctuation

def data_cleaning(sentence):
    
    # removing non-alphabetical characters 
    text = re.sub("[^a-zA-Z]"," ",sentence)
        
    # removing stopwords
    tokens = text.split(" ")
    tokens_final = [word.lower() for word in tokens if not word in final_stopwords]
    
    #lemmatization
    lemma = WordNetLemmatizer()
    lemma_words = [lemma.lemmatize(i) for i in tokens_final]
    
    return (" ").join(lemma_words)

dftrain['Phrase_clean'] = dftrain.loc[:,'Phrase'].apply(lambda row:data_cleaning(row))

df_phrase_senti['Phrase_clean'] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row:data_cleaning(row))

dftrain['Phrase_clean'][:10]

"""<div id='id7'/>

## 4.3. Count Unique words
"""

from collections import Counter

#Count the number of unique words in a phrase
df_phrase_senti['Count_Unique'] = df_phrase_senti.loc[:,'Phrase_clean'].apply(lambda row: len(set(row.split())))

#Making the entire vocubalury with the frecuenty words
word_count = Counter(" ".join(df_phrase_senti['Phrase_clean'].values.tolist()).split(" ")).items()
df_word_count = pd.DataFrame.from_records(list(dict(word_count).items()), columns=['word','count'])
df_word_count.sort_values(by='count', inplace= True, ascending=False)

df_word_count["Percentage"] = (df_word_count['count']/sum(df_word_count['count']))*100
df_word_count["Accumlative"] = df_word_count['Percentage'].cumsum()

print("Total of words:" +str(df_word_count.shape[0]))
df_word_count.head()

df_word_count.tail()

df_phrase_senti['Count_Unique'].hist()
plt.title('Amount of words with preprocessing')
print(df_phrase_senti['Count_Unique'].mean())

"""The average amount of words is 4 per phrase

<div id='id8'/>

## 4.4. Duplicated and null data
"""

df_phrase_senti.Phrase_clean.duplicated().sum()

df_phrase_senti[df_phrase_senti.Phrase_clean.duplicated()].head(2)

df_phrase_senti.loc[7]['Phrase_clean']

df_phrase_senti[df_phrase_senti.Phrase_clean =="escapade demonstrating adage good goose"]

"""## 4.5. Analysis in duplicated data"""

tmp_duplicated = df_phrase_senti[df_phrase_senti.Phrase_clean.duplicated(keep=False)]
tmp_duplicated.head()

tmp_duplicated.Count_Unique.hist()
plt.title('Histogram words in duplicated phrases')

df_phrase_senti.loc[14304]['Phrase']

count_duplicated = tmp_duplicated[['Phrase_clean','Phrase']]
count_duplicated = count_duplicated.groupby(by=['Phrase_clean']).count().reset_index()
count_duplicated.rename(columns={"Phrase":"Count"}, inplace= True)
count_duplicated['Percentage'] =  round((count_duplicated['Count']/dftrain.shape[0])*100,5)
count_duplicated.sort_values(by=["Count",'Percentage'], ascending = False,  inplace=True)
count_duplicated['True_duplicated'] = count_duplicated['Count']-1
count_duplicated.head(5)

# Exporte del df a formato Excel
#from pandas import ExcelWriter
#writer = ExcelWriter('BD_Duplicated.xlsx')
#count_duplicated.to_excel (writer, 'Sheet1' , index = False) 
#writer.save ()

#Todos los valores duplicados
#round((cont_duplicated['True_duplicated'].sum()/dftrain.shape[0])*100,5)
round((df_phrase_senti.Phrase_clean.duplicated().sum()/dftrain.shape[0])*100,5)

#valores sin los espacios vacíos
round((count_duplicated['True_duplicated'].iloc[1:].sum()/dftrain.shape[0])*100,5)

count_duplicated['True_duplicated'].sum()

#pd.set_option("display.max_columns", None)
#cont_duplicated.head(20)

tmp_duplicated.columns

pd.set_option("display.max_columns", None)
tmp_duplicated[tmp_duplicated.Phrase_clean == 'n'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced',kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'n'][['Phrase_clean','Phrase','Sentiment_balanced']].head(20)

pd.set_option("display.max_columns", None)
tmp_duplicated[tmp_duplicated.Phrase_clean == 'movie'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced', kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'movie'][['Phrase_clean','Phrase']].head(10)

pd.set_option("display.max_columns", None)
print(tmp_duplicated[tmp_duplicated.Phrase_clean == 'bad'][['Sentiment_balanced']].value_counts())
tmp_duplicated[tmp_duplicated.Phrase_clean == 'bad'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced', kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'bad'][['Phrase_clean','Phrase']].head(10)

pd.set_option("display.max_columns", None)
print(tmp_duplicated[tmp_duplicated.Phrase_clean == 'better'][['Sentiment_balanced']].value_counts())
tmp_duplicated[tmp_duplicated.Phrase_clean == 'better'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced', kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'better'][['Phrase_clean','Phrase']].head(10)

count_duplicated1 = count_duplicated.iloc[1:35]
#plt.figure(figsize=(10, 5))
f, ax = plt.subplots(figsize=(8, 8))
sns.set_color_codes("pastel")
chart = sns.barplot(x="Percentage", y="Phrase_clean", data=count_duplicated1)
#chart.set_xticklabels(chart.get_xticklabels(), rotation=45)
ax.set(xlim=(0, 0.1), ylabel="", xlabel="Percentage")
plt.title('35th most common duplicated phrases ')
plt.show()

df_phrase_senti['Phrase_clean_senti'] = df_phrase_senti['Sentiment_balanced'].map(str)+'-'+df_phrase_senti['Phrase_clean'] 
dftrain['Phrase_clean_senti'] = dftrain['Sentiment_balanced'].map(str) +'-'+ dftrain['Phrase_clean']

df_phrase_senti['Phrase_clean_senti'].duplicated().sum()

"""There are 69.020 after data clean process with duplicated phrases, includes null values.<br>
There are 54.487 after data clean process with duplicated phrases and the same sentiment, includes null values.<br>
In order to avoid overfitting the duplicated phrases with the same sentiment, will bw delete.

### 4.5.1. Understanding values to delete

- Count duplicated values equal 2
"""

count_duplicated[count_duplicated.Count == 2][['True_duplicated' ]].sum()

"""- Phrase_clean with null data"""

df_phrase_senti[df_phrase_senti.Count_Unique == 0]['Phrase_clean'].count()

print(len(df_phrase_senti.loc[6]['Phrase_clean']))
df_phrase_senti.loc[6]['Phrase_clean']

df_phrase_senti[df_phrase_senti.Count_Unique == 0].head(3)

"""There are 1.303 registers without no words.<br>

<div id='id9'/>

## 4.6. Delete duplicated and null data
"""

#Index for null data
index_drop_null = list(df_phrase_senti[df_phrase_senti.Count_Unique == 0].index)
print('Amount of null: '+str(len(index_drop_null)))

#obtain index for duplicated data
duplicated_two = tmp_duplicated.merge(count_duplicated, left_on='Phrase_clean', right_on='Phrase_clean')
duplicated_two = duplicated_two[duplicated_two.Count == 2]
duplicated_two = duplicated_two.drop_duplicates(subset = ['Phrase_clean'])
index_drop_duplicated = list(duplicated_two.index)
print('Amount of duplicated: '+str(len(index_drop_duplicated)))
duplicated_two.head(2)

# Concat both index list
index_drop = index_drop_null + index_drop_duplicated
print('Amount of duplicated: '+str(len(index_drop)))

"""Delete Null values and only words with 2 instance duplicated"""

print(f'Amount of initail instances:{dftrain.shape[0]}')

dftrain = dftrain.drop(index_drop, axis='index')
print(f'New amount of instances after delete phrases with not words and duplicated:{dftrain.shape[0]}')

plt.figure(figsize=(4, 4))
dftrain['Sentiment_balanced'].value_counts().to_frame().plot(kind = 'bar', color='skyblue')
plt.title('Sentiment_balanced after delete data')
plt.xlabel('Class')
plt.show()

pareto = df_word_count[df_word_count.Accumlative<=80]
pareto1 = pareto.iloc[:20]
pareto.shape[0]

"""Lest take a look for the frecuent words

<font color='red'>These words were part of a initail trial of count words</font>
"""

pareto1.word.unique()

print(df_phrase_senti['Phrase_clean'][df_phrase_senti.Phrase_clean.str.contains("'s")])
print(df_phrase_senti['Phrase_clean'][df_phrase_senti.Phrase_clean.str.contains('``')])
print(df_phrase_senti['Phrase_clean'][df_phrase_senti.Phrase_clean.str.contains('-rrb-')])

df_phrase_senti['Phrase'].loc[1284]

"""<div id='id12'/>

# 6) Model
"""

from tensorflow import keras

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras import regularizers
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras import layers
from keras import regularizers
from keras import backend as K
from keras.callbacks import ModelCheckpoint
from keras.utils.np_utils import to_categorical

texts = np.array(dftrain['Phrase_clean']) 
texts[:10]

# pad sequences
max_length = max([len(s.split()) for s in texts])
max_length

"""This function transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list."""

max_words = df_word_count.shape[0]
max_words

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
comments = pad_sequences(sequences, maxlen=max_length)
print(comments)

y_previo = dftrain['Sentiment_balanced']
y_previo.unique()

y_previo.replace(1, 0, inplace=True)
y_previo.unique()

y_previo.replace(2, 1, inplace=True)
y_previo.unique()

y_previo.replace(3, 2, inplace=True)
y_previo.unique()

y = to_categorical(y_previo)
y

X_train, X_test, y_train, y_test = train_test_split(comments,y, test_size=0.2, random_state=42, stratify=y)

X_train

y_train

"""**input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.

**output_dim:** This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.

**input_length:**This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.

WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/70
"""

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

model1 = Sequential()
model1.add(layers.Embedding(max_words ,max_length)) #The embedding layer
model1.add(layers.LSTM(256,dropout=0.3)) #Our LSTM laye poencias de 2, 256 ,bajar dropout, ver early stopping, batchsize potencia de 2
model1.add(layers.Dense(3,activation='softmax'))


model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])

checkpoint1 = ModelCheckpoint("best_model1.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)

model1.fit(X_train, y_train, epochs=60)

"""#Evaluation"""

scores = model1.evaluate(X_test, y_test, verbose=0)

print("test loss, test acc:", scores)

y_test_original = y_test.argmax(axis = 1)[:,None]
y_pred_original = y_pred.argmax(axis = 1)[:,None]

y_pred_original[:10]

"""SAVE MODEL

"""

!ls "/content/drive/My Drive/Colab Notebooks/Monografía_Especialización/My_Model_RNN"

path

# Guardar el Modelo
model1.save("my_h5_model.h5")

# Recrea exactamente el mismo modelo solo desde el archivo
new_model = keras.models.load_model("my_h5_model.h5")

y_pred_original

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test_original, y_pred_original, target_names=['Class 0', 'Class 1', 'Class 2']))

from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix,precision_score, recall_score,multilabel_confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix

print(f"Accuracy = {accuracy_score(y_test_original,y_pred_original)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test_original,y_pred_original)}")
print(f'F1 Score micro={f1_score(y_test_original,y_pred_original, average="micro")}')
print(f'F1 Score macro={f1_score(y_test_original,y_pred_original, average="macro")}')

matriz = multilabel_confusion_matrix(y_test_original,y_pred_original)

MAT = []
MT = []

for n in range(len(matriz)):
  M = np.asmatrix(matriz[n])
  MAT = np.array([[M[0,0]/len(y_test_original), M[0,1]/len(y_test_original)], [M[1,0]/len(y_test_original), M[1,1]/len(y_test_original)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

