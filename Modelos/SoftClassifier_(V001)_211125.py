# -*- coding: utf-8 -*-
"""EN_SoftClassifier_CarolinaGarcíaP_(V001).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18DYQzlrBJD3LqpuWxWwyqlMJCLRKe8iB

<p><img alt="Colaboratory logo" height="10px" src="https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg" align="left" hspace="10px" vspace="0px",width="100"></p>



<center><h1> Primera Entrega Monografía</h1>
<h2> Carolina García Patiño</h2>
<h3> C.C 1152438081</h3></center>

<br>
<font color='gray'> Problem description, Data cleaning and first models attempts</font>

# TABLE OF CONTENTS

1. [Problem Description](#id1)
2. [Import dataset](#id2)
3. [Data Exploration](#id3)
4. [Preprocessing](#id4)<br>
    4.1 [Imbalaced classes](#id5)<br>
    4.2 [Data cleaning](#id6)<br>
    4.3 [Count Unique words](#id7)<br>
    4.4. [Duplicated and null data](#id8)<br>
    4.5 [Delete duplicated and null data](#id9)
5. [Amount of representative words](#id10)<br>
    5.1[Pareto Analysis](#id11)
6. [Model](#id12)<br>
    6.1 [Multinomial Naive  Bayes](#id3)<br>
      6.1.1. [Evaluation Naive Bayes](#id14)<br>
    6.2 [RandomForest Classifier](#id15)<br>
      6.2.1. [Evaluation Random Forest Classifier](#id16)

<div id='id1'/>

# 1) Problem Description

The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee[1].<br>
Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews

**The sentiment labels are:**

0 - negative <br>
1 - somewhat negative<br>
2 - neutral<br>
3 - somewhat positive<br>
4 - positive<br>

**Import Libraries**
"""

### LIBRERÍAS ESTÁNDAR ###
import csv
import pandas as pd
import numpy as np

## LIBRERÍAS GRÁFICAS ##
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme()

#import googletrans 
#from langdetect import detect

"""<div id='id2'/>

# 2) Import dataset
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive/Colab Notebooks/Monografía_Especialización"

dftrain = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Monografía_Especialización/train.tsv.zip",sep='\t')
dftrain.head()

print(f'Dataset train rows:{dftrain.shape[0]}')

dftrain.dtypes

"""After apply cleanning process, there was an amount of duplicated values including n as product of the descomposition of the word n't. Due to, that word goin to be replace it for the all negation as not."""

dftrain['Phrase'].replace(to_replace=r"\bn't\b", value='not', regex=True, inplace= True)

"""<div id='id3'/>

# 3) Data Exploration

Let see some generally for the columns

**SentenceId**
"""

len(dftrain.SentenceId.unique().tolist())

"""It seems this column could be the identifier for the setences, but there are fewer unique register than total rows.

**Phrase**
"""

plt.hist(dftrain.Phrase.isna().sum())
plt.title('Missing values in Phrase before preprocessing')
plt.show()

print(f'Nun Values in phrase are:{dftrain.Phrase.isnull().sum()}')

print(f'Nun Values in phrase are:{dftrain.Phrase.isna().sum()}')

"""There are not empty phrases in the dataframe"""

plt.hist(dftrain.duplicated(subset=['Phrase']).sum())
plt.title('Duplicated values in Phrase before preprocessing')
plt.show()

"""- Identify phrase's len"""

df_phrase_senti = dftrain[['Phrase','Sentiment']]

df_phrase_senti['Len'] = df_phrase_senti.apply(lambda row: len(row.Phrase), axis=1)

df_phrase_senti.head()

lens = df_phrase_senti.Phrase.str.len()
lens.mean(), lens.std(), lens.max()

lens.hist()
plt.title('Phrases len before preprocessing-Including spaces')

"""The phrases's len withot doing some preprocessing work is around 40 words, that includes the sapces in the phrase. Apparently there no duplicated information.

- Lets take a look phrase's len without spaces
"""

df_phrase_senti['Len_not_spaces'] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row: len(row.replace(" ", "")))
df_phrase_senti['Len_not_spaces'].mean(), df_phrase_senti['Len_not_spaces'].std(), df_phrase_senti['Len_not_spaces'].max()

lens2 = df_phrase_senti.Phrase.replace(" ","").str.len()
lens2.hist()
plt.title('Phrases len without spaces')

"""The phrases's len withot spaces is around 34 words, the distribution is similary that the previous one. Now, appears 62 duplicated phrases."""

df_phrase_senti.Phrase.str.replace(" ","").isnull().sum()

plt.hist(df_phrase_senti.Phrase.str.replace(" ","").isnull().sum())
plt.title('Missing values in without spaces')
plt.show()

plt.hist(df_phrase_senti.Phrase.str.replace(" ","").duplicated().sum())
plt.title('Duplicated values in without spaces')
plt.show()

tmp = df_phrase_senti.Phrase.str.replace(" ","")
tmp.duplicated(keep= False).sum()

"""- Count words in a sentence"""

df_phrase_senti['amount_words'] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row: len(row.split()))

df_phrase_senti['amount_words'].hist()
plt.title('Amount of words without preprocessing')

df_phrase_senti['amount_words'].mean()

df_phrase_senti[df_phrase_senti['amount_words']==0]

"""The average amount of words is 7 per phrase.<br>
There is a one register without no words.

- Identify Language
"""

def language_detect(sentence):
    try:
        language = detect(sentence)
    except:
        language= "error"
    return language

#df_phrase_senti["Language"] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row: language_detect(row))
#df_phrase_senti.shape

#df_phrase_senti.Language.value_counts()

#df_phrase_senti[df_phrase_senti["Language"] =='it'].head()

#df_phrase_senti[df_phrase_senti["Language"] =='fr'].head()

#df_phrase_senti[df_phrase_senti["Language"] =='fr'].Len.unique()

"""**Sentiment**"""

print(f'Nun Values in Sentiment are:{dftrain.Sentiment.isnull().sum()}')

dftrain.Sentiment.value_counts().to_frame().plot(kind = 'bar')

dftrain.Sentiment.value_counts()

"""There is a problem for imbalace classes. The neutral class represent 51%, 21% some positive ,17% some negative, 6% positive and 5% negative.

The strategy would be two for solve the problem:
1) To add the negative and some negative classes into negative and positives and some positive into positive. <br>
2) Implemate Smoteen strategy, creating sintetic samples, to balance classes.

<div id='id4'/>

# 4) Preprocessing

<div id='id5'/>

## 4.1. Imbalaced classes
"""

plt.figure(figsize=(4, 4))
dftrain.Sentiment.value_counts().to_frame().plot(kind = 'bar', color='teal')
plt.show()

tmp_class = pd.DataFrame(dftrain.Sentiment.value_counts()).sort_index()
tmp_class['Percentage'] = round(tmp_class['Sentiment']/sum(tmp_class['Sentiment'])*100,2)
tmp_class

def balanced_classes(class_init):
    if class_init == 0:
        value = 1
    elif class_init == 4:
        value = 3
    else:
        value = class_init
        
    return value

df_phrase_senti['Sentiment_balanced'] = df_phrase_senti.loc[:,'Sentiment'].apply(lambda row: balanced_classes(row) )

plt.figure(figsize=(4, 4))
df_phrase_senti['Sentiment_balanced'].value_counts().to_frame().plot(kind = 'bar', color='skyblue')
plt.xlabel('Class')
plt.show()

tmp2_class = pd.DataFrame(df_phrase_senti.Sentiment_balanced.value_counts()).sort_index()
tmp2_class['Percentage'] = round(tmp2_class['Sentiment_balanced']/sum(tmp2_class['Sentiment_balanced'])*100,2)
tmp2_class

dftrain['Sentiment_balanced'] = dftrain.loc[:,'Sentiment'].apply(lambda row: balanced_classes(row) )

"""<div id='id6'/>

## 4.2. Data cleaning

In a first attempt, the data cleaning process only takes remong stopwords, but this process let some wordas as n't, that are identifued as a contraction of a negation word in english lenguage. For that reason, in a second attempt lemmatization is implemented to reduce that kind of issue.
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

nltk.download('wordnet')

import re
from nltk.stem import WordNetLemmatizer

my_stopwords = list(stopwords.words('english'))

my_stopwords[:10]

import string 
punctuation = list(string.punctuation)

"""Add some extra special characteres extract from a previous exploration in the counter words fase."""

punctuation.append('--')
punctuation.append('...')
punctuation.append('``')
punctuation.append('')

punctuation[:10]

final_stopwords = my_stopwords + punctuation

def data_cleaning(sentence):
    
    # removing non-alphabetical characters 
    text = re.sub("[^a-zA-Z]"," ",sentence)
        
    # removing stopwords
    tokens = text.split(" ")
    tokens_final = [word.lower() for word in tokens if not word in final_stopwords]
    
    #lemmatization
    lemma = WordNetLemmatizer()
    lemma_words = [lemma.lemmatize(i) for i in tokens_final]
    
    return (" ").join(lemma_words)

dftrain['Phrase_clean'] = dftrain.loc[:,'Phrase'].apply(lambda row:data_cleaning(row))

df_phrase_senti['Phrase_clean'] = df_phrase_senti.loc[:,'Phrase'].apply(lambda row:data_cleaning(row))

dftrain['Phrase_clean'][:10]

"""<div id='id7'/>

## 4.3. Count Unique words
"""

from collections import Counter

#Count the number of unique words in a phrase
df_phrase_senti['Count_Unique'] = df_phrase_senti.loc[:,'Phrase_clean'].apply(lambda row: len(set(row.split())))

#Making the entire vocubalury with the frecuenty words
word_count = Counter(" ".join(df_phrase_senti['Phrase_clean'].values.tolist()).split(" ")).items()
df_word_count = pd.DataFrame.from_records(list(dict(word_count).items()), columns=['word','count'])
df_word_count.sort_values(by='count', inplace= True, ascending=False)

df_word_count["Percentage"] = (df_word_count['count']/sum(df_word_count['count']))*100
df_word_count["Accumlative"] = df_word_count['Percentage'].cumsum()

print("Total of words:" +str(df_word_count.shape[0]))
df_word_count.head()

df_word_count.tail()

df_phrase_senti['Count_Unique'].hist()
plt.title('Amount of words with preprocessing')
print(df_phrase_senti['Count_Unique'].mean())

"""The average amount of words is 4 per phrase

<div id='id8'/>

## 4.4. Duplicated and null data
"""

df_phrase_senti.Phrase_clean.duplicated().sum()

df_phrase_senti[df_phrase_senti.Phrase_clean.duplicated()].head(2)

df_phrase_senti.loc[7]['Phrase_clean']

df_phrase_senti[df_phrase_senti.Phrase_clean =="escapade demonstrating adage good goose"]

"""## 4.5. Analysis in duplicated data"""

tmp_duplicated = df_phrase_senti[df_phrase_senti.Phrase_clean.duplicated(keep=False)]
tmp_duplicated.head()

tmp_duplicated.Count_Unique.hist()
plt.title('Histogram words in duplicated phrases')

df_phrase_senti.loc[14304]['Phrase']

count_duplicated = tmp_duplicated[['Phrase_clean','Phrase']]
count_duplicated = count_duplicated.groupby(by=['Phrase_clean']).count().reset_index()
count_duplicated.rename(columns={"Phrase":"Count"}, inplace= True)
count_duplicated['Percentage'] =  round((count_duplicated['Count']/dftrain.shape[0])*100,5)
count_duplicated.sort_values(by=["Count",'Percentage'], ascending = False,  inplace=True)
count_duplicated['True_duplicated'] = count_duplicated['Count']-1
count_duplicated.head(5)

# Exporte del df a formato Excel
#from pandas import ExcelWriter
#writer = ExcelWriter('BD_Duplicated.xlsx')
#count_duplicated.to_excel (writer, 'Sheet1' , index = False) 
#writer.save ()

#Todos los valores duplicados
#round((cont_duplicated['True_duplicated'].sum()/dftrain.shape[0])*100,5)
round((df_phrase_senti.Phrase_clean.duplicated().sum()/dftrain.shape[0])*100,5)

#valores sin los espacios vacíos
round((count_duplicated['True_duplicated'].iloc[1:].sum()/dftrain.shape[0])*100,5)

count_duplicated['True_duplicated'].sum()

#pd.set_option("display.max_columns", None)
#cont_duplicated.head(20)

tmp_duplicated.columns

pd.set_option("display.max_columns", None)
tmp_duplicated[tmp_duplicated.Phrase_clean == 'n'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced',kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'n'][['Phrase_clean','Phrase','Sentiment_balanced']].head(20)

pd.set_option("display.max_columns", None)
tmp_duplicated[tmp_duplicated.Phrase_clean == 'movie'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced', kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'movie'][['Phrase_clean','Phrase']].head(10)

pd.set_option("display.max_columns", None)
print(tmp_duplicated[tmp_duplicated.Phrase_clean == 'bad'][['Sentiment_balanced']].value_counts())
tmp_duplicated[tmp_duplicated.Phrase_clean == 'bad'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced', kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'bad'][['Phrase_clean','Phrase']].head(10)

pd.set_option("display.max_columns", None)
print(tmp_duplicated[tmp_duplicated.Phrase_clean == 'better'][['Sentiment_balanced']].value_counts())
tmp_duplicated[tmp_duplicated.Phrase_clean == 'better'][['Sentiment_balanced']].value_counts().to_frame().reset_index().rename(columns= {0: 'Count'}).plot(x = 'Sentiment_balanced', kind = 'bar', color='skyblue')
tmp_duplicated[tmp_duplicated.Phrase_clean == 'better'][['Phrase_clean','Phrase']].head(10)

count_duplicated1 = count_duplicated.iloc[1:35]
#plt.figure(figsize=(10, 5))
f, ax = plt.subplots(figsize=(8, 8))
sns.set_color_codes("pastel")
chart = sns.barplot(x="Percentage", y="Phrase_clean", data=count_duplicated1)
#chart.set_xticklabels(chart.get_xticklabels(), rotation=45)
ax.set(xlim=(0, 0.1), ylabel="", xlabel="Percentage")
plt.title('35th most common duplicated phrases ')
plt.show()

df_phrase_senti['Phrase_clean_senti'] = df_phrase_senti['Sentiment_balanced'].map(str)+'-'+df_phrase_senti['Phrase_clean'] 
dftrain['Phrase_clean_senti'] = dftrain['Sentiment_balanced'].map(str) +'-'+ dftrain['Phrase_clean']

df_phrase_senti['Phrase_clean_senti'].duplicated().sum()

"""There are 69.020 after data clean process with duplicated phrases, includes null values.<br>
There are 54.487 after data clean process with duplicated phrases and the same sentiment, includes null values.<br>
In order to avoid overfitting the duplicated phrases with the same sentiment, will bw delete.

### 4.5.1. Understanding values to delete

- Count duplicated values equal 2
"""

count_duplicated[count_duplicated.Count == 2][['True_duplicated' ]].sum()

"""- Phrase_clean with null data"""

df_phrase_senti[df_phrase_senti.Count_Unique == 0]['Phrase_clean'].count()

print(len(df_phrase_senti.loc[6]['Phrase_clean']))
df_phrase_senti.loc[6]['Phrase_clean']

df_phrase_senti[df_phrase_senti.Count_Unique == 0].head(3)

"""There are 1.303 registers without no words.<br>

<div id='id9'/>

## 4.6. Delete duplicated and null data
"""

#Index for null data
index_drop_null = list(df_phrase_senti[df_phrase_senti.Count_Unique == 0].index)
print('Amount of null: '+str(len(index_drop_null)))

#obtain index for duplicated data
duplicated_two = tmp_duplicated.merge(count_duplicated, left_on='Phrase_clean', right_on='Phrase_clean')
duplicated_two = duplicated_two[duplicated_two.Count == 2]
duplicated_two = duplicated_two.drop_duplicates(subset = ['Phrase_clean'])
index_drop_duplicated = list(duplicated_two.index)
print('Amount of duplicated: '+str(len(index_drop_duplicated)))
duplicated_two.head(2)

# Concat both index list
index_drop = index_drop_null + index_drop_duplicated
print('Amount of duplicated: '+str(len(index_drop)))

"""Delete Null values and only words with 2 instance duplicated"""

print(f'Amount of initail instances:{dftrain.shape[0]}')

dftrain = dftrain.drop(index_drop, axis='index')
print(f'New amount of instances after delete phrases with not words and duplicated:{dftrain.shape[0]}')

plt.figure(figsize=(4, 4))
dftrain['Sentiment_balanced'].value_counts().to_frame().plot(kind = 'bar', color='skyblue')
plt.title('Sentiment_balanced after delete data')
plt.xlabel('Class')
plt.show()

"""<div id='id10'/>

# 5) Amount of representative words
"""

pareto = df_word_count[df_word_count.Accumlative<=80]
pareto1 = pareto.iloc[:20]
pareto.shape[0]

"""Lest take a look for the frecuent words

<font color='red'>These words were part of a initail trial of count words</font>
"""

pareto1.word.unique()

print(df_phrase_senti['Phrase_clean'][df_phrase_senti.Phrase_clean.str.contains("'s")])
print(df_phrase_senti['Phrase_clean'][df_phrase_senti.Phrase_clean.str.contains('``')])
print(df_phrase_senti['Phrase_clean'][df_phrase_senti.Phrase_clean.str.contains('-rrb-')])

df_phrase_senti['Phrase'].loc[1284]

"""<div id='id11'/>

## 5.1. Pareto Analysis
"""

plt.figure(figsize=(10, 5))
chart = sns.barplot(x="word", y="Percentage", data=pareto1)
chart.set_xticklabels(chart.get_xticklabels(), rotation=45)
plt.title('20th most common words ')
plt.show()

print(f'The 80% of the words are represented for: {pareto.shape[0]}')

"""<div id='id12'/>

# 6) Model

Model libraries
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB, GaussianNB,ComplementNB
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix,make_scorer
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix,precision_score, recall_score,multilabel_confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix

"""### 6.1. TfidfVectorizer"""

nltk.download('punkt')

import itertools

texts = np.array(dftrain['Phrase_clean'])

tfidf_vectorizer = TfidfVectorizer(max_features=df_word_count.shape[0],stop_words='english',
                                   use_idf=True, ngram_range=(1,3))

# Commented out IPython magic to ensure Python compatibility.
# %time tfidf_matrix = tfidf_vectorizer.fit_transform(dftrain['Phrase_clean']) #fit the vectorizer to synopses

print(tfidf_matrix.shape)
print(tfidf_matrix)

y = dftrain['Sentiment_balanced']

"""<div id='id13'/>

## 6.2. Multinomial Naive  Bayes
"""

X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix,y, test_size=0.2, random_state=42, stratify=y)
st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

nb_classifier = MultinomialNB()

y.value_counts()

"""Creating the GridSearchCV object"""

parameters={'alpha':[0.1, 0.5,1],'class_prior':[[0.35,0.3,0.35],[0.4,0.3,0.3],[0.4,0.2,0.4]]}
f1 = make_scorer(f1_score , average='weighted')

nb_clf = GridSearchCV(estimator=nb_classifier, param_grid=parameters, cv=st, scoring=f1)

nb_clf.fit(X_train,y_train)
y_pred = nb_clf.predict(X_test)

"""### 6.2.1. Best Estimator"""

nb_clf.best_estimator_

"""<div id='id14'/>

### 6.2.2. Naive Bayes Evaluation
"""

from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix,precision_score, recall_score,multilabel_confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix

print(f"Accuracy = {accuracy_score(y_test,y_pred)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test,y_pred)}")
print(f'F1 Score micro={f1_score(y_test, y_pred, average="micro")}')
print(f'F1 Score macro={f1_score(y_test, y_pred, average="macro")}')

matriz = multilabel_confusion_matrix(y_test, y_pred)

y_test.value_counts

MAT = []
MT = []

for n in range(len(matriz)):
  M = np.asmatrix(matriz[n])
  MAT = np.array([[M[0,0]/len(y_test), M[0,1]/len(y_test)], [M[1,0]/len(y_test), M[1,1]/len(y_test)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3']))

"""The genral F1-score for this first attempt, is 0.48 with is nearly to obtain at least the middle of the predictions good. In comparisson with the leadboard for the kaggle competion there is a deviation of 0,28461.
Something to add, is the neutral class obtains the f1-score over the other classes, despite has the biggest amount of instances.

## 6.3. Gaussian Naive
"""

GN_classifier = GaussianNB()

tfidf_vectorizer_GN = TfidfVectorizer(max_features=df_word_count.shape[0],stop_words='english',
                                   use_idf=True, ngram_range=(1,2))

tfidf_matrix_GN = tfidf_vectorizer_GN.fit_transform(dftrain['Phrase_clean'])
y = dftrain['Sentiment_balanced']

from sklearn.utils import resample
tfidf_matrix_GN2, y_GN = resample(tfidf_matrix_GN, y,  n_samples=10000, random_state=123)
tfidf_matrix_GN2 = tfidf_matrix_GN2.toarray()

X_train_GN, X_test_GN, y_train_GN, y_test_GN = train_test_split(tfidf_matrix_GN2,y_GN, test_size=0.2, random_state=42, stratify=y_GN)

st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

GN_classifier.fit(X_train_GN, y_train_GN)

y_pred_GN = GN_classifier.predict(X_test_GN)

"""### 6.3.1. Gaussian Naive Evaluation

"""

matriz_GN = multilabel_confusion_matrix(y_test_GN, y_pred_GN)

print(f"Accuracy = {accuracy_score(y_test_GN,y_pred_GN)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test_GN,y_pred_GN)}")
print(f'F1 Score={f1_score(y_test_GN, y_pred_GN, average="weighted")}')

MAT = []
MT = []

for n in range(len(matriz_GN)):
  M = np.asmatrix(matriz_GN[n])
  MAT = np.array([[M[0,0]/len(y_test_GN), M[0,1]/len(y_test_GN)], [M[1,0]/len(y_test_GN), M[1,1]/len(y_test_GN)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test_GN, y_pred_GN, target_names=['Class 1', 'Class 2', 'Class 3']))

"""<div id='id15'/>

## 6.4. RandomForest Classifier
"""

from sklearn.ensemble import RandomForestClassifier

X_train_RF, X_test_RF, y_train_RF, y_test_RF = train_test_split(tfidf_matrix,y, test_size=0.2, random_state=42, stratify=y)
st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

clf_b = RandomForestClassifier(max_depth=2, class_weight='balanced_subsample')
f1 = make_scorer(f1_score , average='weighted')

parameters = {'n_estimators':[60,80,100,200,300,400], 'max_depth':[2,4,6,8,10,20]}

rf_clf = GridSearchCV(estimator=clf_b, param_grid=parameters, cv=st, scoring=f1)

rf_clf.fit(X_train_RF,y_train_RF)

rf_clf.best_estimator_

y_pred_RF = rf_clf.predict(X_test_RF)

"""<div id='id16'/>

### 6.4.1 Random Forest Classifier Evaluation
"""

print(f"Accuracy = {accuracy_score(y_test_RF,y_pred_RF)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test_RF,y_pred_RF)}")
print(f'F1 Score micro={f1_score(y_test_RF, y_pred_RF, average="micro")}')
print(f'F1 Score macro={f1_score(y_test_RF, y_pred_RF, average="macro")}')

matriz_RF = multilabel_confusion_matrix(y_test_RF, y_pred_RF)

type(y_test_RF)

y_test_RF.to_frame().value_counts()

MAT = []
MT = []

for n in range(len(matriz_RF)):
  M = np.asmatrix(matriz_RF[n])
  MAT = np.array([[M[0,0]/len(y_test_RF), M[0,1]/len(y_test_RF)], [M[1,0]/len(y_test_RF), M[1,1]/len(y_test_RF)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test_RF, y_pred_RF, target_names=['Class 1', 'Class 2', 'Class 3']))

"""In general and in echa class, this represents a worst metric than the Naive Bayes.

## 6.5. KNN
"""

X_train_KNN, X_test_KNN, y_train_KNN, y_test_KNN = train_test_split(tfidf_matrix,y, 
                                                                    test_size=0.2, random_state=42, stratify=y)

st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

from sklearn.neighbors import KNeighborsClassifier

KNN_classifier = KNeighborsClassifier()

parameters ={'n_neighbors':[ 5, 7, 9, 11,13,15], 'metric':['minkowski'], 'p':[1,2]}
f1 = make_scorer(f1_score , average='weighted')
KNN_clf = GridSearchCV(estimator=KNN_classifier, param_grid=parameters, cv=st, scoring=f1)

KNN_clf = GridSearchCV(estimator=KNN_classifier, param_grid=parameters, cv=st, scoring=f1)

KNN_clf.fit(X_train_KNN,y_train_KNN)

"""### 6.5.1. Best Estimator"""

KNN_clf.best_estimator_

"""### 6.5.2. KNN Evaluation"""

y_pred_KNN = KNN_clf.predict(X_test_KNN)

print(f"Accuracy = {accuracy_score(y_test_KNN,y_pred_KNN)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test_KNN,y_pred_KNN)}")
print(f'F1 Score micro={f1_score(y_test_KNN, y_pred_KNN, average="micro")}')
print(f'F1 Score macro={f1_score(y_test_KNN, y_pred_KNN, average="macro")}')

matriz_KNN = multilabel_confusion_matrix(y_test_KNN, y_pred_KNN)

MAT = []
MT = []

for n in range(len(matriz_KNN)):
  M = np.asmatrix(matriz_KNN[n])
  MAT = np.array([[M[0,0]/len(y_test_KNN), M[0,1]/len(y_test_KNN)], [M[1,0]/len(y_test_KNN), M[1,1]/len(y_test_KNN)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

from sklearn.metrics import classification_report

print('\nClassification Report\n')
print(classification_report(y_test_KNN, y_pred_KNN, target_names=['Class 1', 'Class 2', 'Class 3']))

"""## 6.6 SVM"""

from sklearn import svm

X_train_SVM, X_test_SVM, y_train_SVM, y_test_SVM = train_test_split(tfidf_matrix,y, test_size=0.2, random_state=42, stratify=y)
st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

y_test_SVM.value_counts()

"""Polinmial"""

SVM_Poly =  svm.SVC(kernel='poly')
parameters ={'degree':[2, 3], 'C':[ 0.5, 1]}
f1 = make_scorer(f1_score , average='weighted')
SVM_Poly = GridSearchCV(estimator=SVM_Poly, param_grid=parameters, cv=st, scoring=f1)

SVM_Poly.fit(X_train_SVM,y_train_SVM)

"""RBF"""

SVM_RBF =  svm.SVC(kernel='rbf')
parameters ={'gamma':[ 1,5,10], 'C':[0.5,1]} #gamma 1,5,10 regulariza 0.5 y 1
f1 = make_scorer(f1_score , average='weighted')
SVM_RBF = GridSearchCV(estimator=SVM_RBF, param_grid=parameters, cv=st , scoring=f1)

SVM_RBF.fit(X_train_SVM,y_train_SVM)

"""### 6.6.1 Best Estimotors"""

SVM_Poly.best_Estimator

SVM_RBF.best_Estimator

"""###6.6.2 SVM Evaluation

#### Polinomial
"""

y_pred_SVM_Poly = SVM_Poly.predict(X_test_SVM)

"""## 6.6 Voting Classifier"""

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm

clf1 = RandomForestClassifier(n_estimators=300, random_state=1,max_depth=20)
clf2 = AdaBoostClassifier(n_estimators=400)
clf3 = KNeighborsClassifier(metric='minkowski',n_neighbors=5, p=2)
#clf4 = svm.SVC(kernel='poly', degree=3, C=1)#dgreee max 5, c seprabilidad 0.5-5
#clf5 = svm.SVC(kernel='rbf', gamma=0.5, C=0.1)#gamma 1-10,función base radial

voting_clf_base = VotingClassifier(estimators=[('rf', clf1), ('ab', clf2),('knn', clf3)], voting='hard')

X_train_vt, X_test_vt, y_train_vt, y_test_vt = train_test_split(tfidf_matrix,y, test_size=0.2, random_state=42, stratify=y)
st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

voting_clf = voting_clf_base.fit(X_train_vt, y_train_vt)

y_pred_vt = voting_clf.predict(X_test_vt)

"""### 6.6.2. Evaluation"""

matriz_vt = multilabel_confusion_matrix(y_test_vt, y_pred_vt)

print(f"Accuracy = {accuracy_score(y_test_vt, y_pred_vt)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test_vt, y_pred_vt)}")
print(f'F1 Score micro={f1_score(y_test_vt, y_pred_vt, average="micro")}')
print(f'F1 Score macro={f1_score(y_test_vt, y_pred_vt, average="macro")}')

MAT = []
MT = []

for n in range(len(matriz_vt)):
  M = np.asmatrix(matriz_vt[n])
  MAT = np.array([[M[0,0]/len(y_test_vt), M[0,1]/len(y_test_vt)], [M[1,0]/len(y_test_vt), M[1,1]/len(y_test_vt)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test_vt, y_pred_vt, target_names=['Class 1', 'Class 2', 'Class 3']))

"""## 6.7 AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier

X_train_ab, X_test_ab, y_train_ab, y_test_ab = train_test_split(tfidf_matrix,y, test_size=0.2, random_state=42, stratify=y)
st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

parameters = {'n_estimators':[100,200,300,400]}
f1 = make_scorer(f1_score , average='weighted')

clf_ab = AdaBoostClassifier()
ab_clf = GridSearchCV(estimator=clf_ab, param_grid=parameters, cv=st, scoring=f1)

ab_clf = ab_clf.fit(X_train_ab, y_train_ab)

"""### 6.7.1 Best Estimator"""

ab_clf.best_estimator_

y_pred_ab = ab_clf.predict(X_test_ab)

matriz_ab = multilabel_confusion_matrix(y_test_ab, y_pred_ab)

print(f"Accuracy = {accuracy_score(y_test_ab, y_pred_ab)}")
print(f"Balanced Accuracy = {balanced_accuracy_score(y_test_ab, y_pred_ab)}")
print(f'F1 Score micro={f1_score(y_test_ab, y_pred_ab, average="micro")}')
print(f'F1 Score macro={f1_score(y_test_ab, y_pred_ab, average="macro")}')

MAT = []
MT = []

for n in range(len(matriz_ab)):
  M = np.asmatrix(matriz_ab[n])
  MAT = np.array([[M[0,0]/len(y_test_ab), M[0,1]/len(y_test_ab)], [M[1,0]/len(y_test_ab), M[1,1]/len(y_test_ab)]])
  MT.append(MAT)
  Class_Label = n + 1
  group_counts = ["{0:0.0f}".format(value) for value in (np.asarray(M)).flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in MT[n].flatten()]
  group_names  = ['True Neg','False Pos','False Neg','True Pos']
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)

  ax= plt.subplot()
  sns.heatmap(MT[n], cmap="Blues",annot=labels,fmt="", xticklabels= ['Other Class',"Clase %d" % Class_Label], yticklabels=['Other Class',"Clase %d" % Class_Label])
  ax.set_title(f'Confusion matrix Class {Class_Label}')
  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels'); 
  #ax.tick_params(right=False, top=True, labelright=False, labeltop=True) 
  print(" ")
  print(" ")
  plt.show()

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test_ab, y_pred_ab, target_names=['Class 1', 'Class 2', 'Class 3']))

